{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $$y = 2x^2$$\n",
    "### $$\\frac{dy}{dx} = 4x$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(12.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# tensorflow的梯度带一般只能对变量求导，这样是为了优化训练过程\n",
    "x = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape() as Tape:\n",
    "    y = 2 * x**2\n",
    "\n",
    "d_yx = Tape.gradient(y, x)\n",
    "print (d_yx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tf.Tensor(12.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 对常量求导需要用到tape.watch\n",
    "x = tf.constant(3.0)\n",
    "\n",
    "with tf.GradientTape() as Tape:\n",
    "    y = 2 * x**2\n",
    "d_yx = Tape.gradient(y, x)\n",
    "print (d_yx)\n",
    "\n",
    "with tf.GradientTape() as Tape:\n",
    "    Tape.watch(x)\n",
    "    y = 2 * x**2\n",
    "d_yx = Tape.gradient(y, x)\n",
    "print (d_yx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $$y=2x^2$$\n",
    "### $$\\frac{d^2y}{dx^2} = 4$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(4.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 还可以求高阶导数\n",
    "x = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape() as Tape1:\n",
    "    with tf.GradientTape() as Tape2:\n",
    "        y = 2 * x**2\n",
    "    d_yx = Tape2.gradient(y, x)\n",
    "d2_yx2 = Tape1.gradient(d_yx, x)\n",
    "print (d2_yx2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何用优化器进行梯度下降，这里用SGD优化举例。\n",
    "\n",
    "根据上一节提出的公式：参数的变化量为：\n",
    "\n",
    "### $$\\Delta w = -lr * \\triangledown w$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用公式推出的新x： tf.Tensor(2.88, shape=(), dtype=float32)\n",
      "用优化器更新得到的x： <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.88>\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "SGD_opt = tf.keras.optimizers.SGD(learning_rate)\n",
    "\n",
    "x = tf.Variable(3.0)\n",
    "with tf.GradientTape() as Tape:\n",
    "    y = 2 * x**2\n",
    "d_y = Tape.gradient(y, x)\n",
    "print('用公式推出的新x：', x - learning_rate * d_y)\n",
    "\n",
    "SGD_opt.apply_gradients(zip([d_y], [x]))\n",
    "print('用优化器更新得到的x：', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们来看看如何将梯度带用到模型的训练中，然后有请我们的老朋友MNIST。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist_data\\train-images-idx3-ubyte.gz\n",
      "Extracting mnist_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting mnist_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist_data\\t10k-labels-idx1-ubyte.gz\n",
      "开始训练\n",
      "steps: 1900, losses: 0.054487"
     ]
    }
   ],
   "source": [
    "import input_data\n",
    "\n",
    "dataset = input_data.read_data_sets('mnist_data', one_hot=True)\n",
    "\n",
    "input_ = tf.keras.Input(shape=(784, ))\n",
    "dense = tf.keras.layers.Dense(128, activation='relu')(input_)\n",
    "out = tf.keras.layers.Dense(10, activation='softmax')(dense)\n",
    "\n",
    "model = tf.keras.Model(inputs=input_, outputs=out)\n",
    "optimizor = tf.keras.optimizers.Adam()\n",
    "\n",
    "total_steps = 2000\n",
    "batch_size = 128\n",
    "print('开始训练')\n",
    "for i in range(total_steps):\n",
    "    train_image, train_label = dataset.train.next_batch(batch_size)\n",
    "    with tf.GradientTape() as tape:\n",
    "        out = model(train_image)\n",
    "        loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(train_label, out))\n",
    "    grad = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizor.apply_gradients(zip(grad, model.trainable_variables))\n",
    "    if i % 100 == 0:\n",
    "        print ('\\rsteps: %d, losses: %f' %(i, loss.numpy()), end='')\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit ('tf2': conda)",
   "language": "python",
   "name": "python36864bittf2condaa3c1267118644d0dab19695901f139a7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
